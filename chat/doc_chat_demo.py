import argparse
import logging
from dataclasses import dataclass

import streamlit as st
from magic_doc.docconv import DocConverter
from openai import OpenAI

# Set up logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')


@dataclass
class GenerationConfig:
    # this config is used for chat to provide more diversity
    max_tokens: int = 1024
    top_p: float = 1.0
    temperature: float = 0.1
    repetition_penalty: float = 1.005


def generate(
    client,
    messages,
    generation_config,
):
    stream = client.chat.completions.create(
        model=st.session_state['model_name'],
        messages=messages,
        stream=True,
        temperature=generation_config.temperature,
        top_p=generation_config.top_p,
        max_tokens=generation_config.max_tokens,
        frequency_penalty=generation_config.repetition_penalty,
    )
    return stream


def prepare_generation_config():
    with st.sidebar:
        max_tokens = st.number_input('Max Tokens',
                                     min_value=100,
                                     max_value=4096,
                                     value=1024)
        top_p = st.number_input('Top P', 0.0, 1.0, 1.0, step=0.01)
        temperature = st.number_input('Temperature', 0.0, 1.0, 0.05, step=0.01)
        repetition_penalty = st.number_input('Repetition Penalty',
                                             0.8,
                                             1.2,
                                             1.02,
                                             step=0.001,
                                             format='%0.3f')
        st.button('æ¸…ç©ºå†å²è®°å½•', on_click=on_btn_click)
        # ç»™å‡ºä¸€ä¸ªè®¾ç½® system prompt çš„æ¡†å’ŒæŒ‰é’®
        st.text_area('System Prompt', value=main_prompt, height=400, key='new_prompt')
        st.button('æ›´æ–° System Prompt', on_click=on_prompt_btn_click)
 


    generation_config = GenerationConfig(max_tokens=max_tokens,
                                         top_p=top_p,
                                         temperature=temperature,
                                         repetition_penalty=repetition_penalty)

    return generation_config


def on_btn_click():
    del st.session_state.messages
    st.session_state.file_content_found = False
    st.session_state.file_content_used = False

def on_prompt_btn_click():
    # è·å– st.text_area('System Prompt', value=main_prompt, height=400) çš„æ–‡å­—
    main_prompt = st.session_state["new_prompt"]
    st.session_state.messages = [{
        'role': 'system',
        'content': main_prompt,
    }]


user_avator = 'InternLM/assets/user.png'
robot_avator = 'InternLM/assets/robot.png'

st.title('ä½ çš„ä¸ªäººå°åŠ©æ‰‹ ğŸ“')

main_prompt = """è¯·æ‰®æ¼”ä¸€ä¸ªå…·æœ‰**æ¸©æš–ã€è€å¿ƒå’Œå…³æ€€æ€åº¦**çš„æ™ºèƒ½é™ªä¼´åŠ©æ‰‹ï¼Œä¸“ä¸ºå­¤ç‹¬çš„è€äººè®¾è®¡ï¼Œå¸®åŠ©ä»–ä»¬è§£ç­”é—®é¢˜å¹¶æä¾›æƒ…æ„Ÿæ”¯æŒã€‚ä½ éœ€è¦åšåˆ°ä»¥ä¸‹å‡ ç‚¹ï¼š

1. **è‡ªç„¶å¯¹è¯**ï¼šç”¨äº²åˆ‡ã€é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ä¸ç”¨æˆ·äº¤æµï¼Œä¿æŒè‡ªç„¶æµç•…çš„å¯¹è¯é£æ ¼ã€‚
2. **æƒ…æ„Ÿå›åº”**ï¼šå¯¹è€äººçš„æé—®æˆ–æƒ…ç»ªè¡¨ç°å‡ºçœŸè¯šçš„ç†è§£ã€å…³å¿ƒå’Œé¼“åŠ±ï¼Œä¼ é€’æ¸©æš–å’Œé™ªä¼´æ„Ÿã€‚
3. **æœ‰æ„ä¹‰çš„ç­”æ¡ˆ**ï¼šè§£ç­”è€å¹´äººæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ç–‘æƒ‘ï¼Œå¦‚å¥åº·å»ºè®®ã€å…´è¶£çˆ±å¥½ã€ç”Ÿæ´»ä¹ æƒ¯ç­‰ï¼Œå¹¶æä¾›è´´å¿ƒã€å®é™…çš„å»ºè®®ï¼Œå¸®åŠ©ç¼“è§£ä»–ä»¬çš„å­¤ç‹¬æ„Ÿã€‚
"""


def main(base_url):
    # Initialize the client for the model
    client = OpenAI(base_url=base_url, api_key="not-needed", timeout=12000)

    # Get the model ID
    model_name = client.models.list().data[0].id
    st.session_state['model_name'] = model_name

    # Get the generation config
    generation_config = prepare_generation_config()

    # Initialize session state
    if 'messages' not in st.session_state:
        st.session_state.messages = [{
        'role': 'system',
        'content': main_prompt,
    }]
    

    # Display chat messages
    for message in st.session_state.messages:
        if message['role'] == 'system':
            continue
              
        with st.chat_message(message['role'], avatar=message.get('avatar')):
            st.markdown(message['content'])

    # Handle user input and response generation
    if prompt := st.chat_input("è¯·é—®æ‚¨æœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©?"):
        turn = {'role': 'user', 'content': prompt, 'avatar': user_avator}

        st.session_state.messages.append(turn)
        with st.chat_message('user', avatar=user_avator):
            st.markdown(prompt)

        with st.chat_message('assistant', avatar=robot_avator):
            messages = [{
                'role':
                m['role'],
                'content':
                m['merged_content'] if 'merged_content' in m else m['content'],
            } for m in st.session_state.messages]
            # Log messages to the terminal
            for m in messages:
                logging.info(
                    f"\n\n*** [{m['role']}] ***\n\n\t{m['content']}\n\n")
            stream = generate(client, messages, generation_config)
            response = st.write_stream(stream)
        st.session_state.messages.append({
            'role': 'assistant',
            'content': response,
            'avatar': robot_avator
        })


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Run Streamlit app with OpenAI client.')
    parser.add_argument('--base_url',
                        type=str,
                        required=True,
                        help='Base URL for the OpenAI client')
    args = parser.parse_args()
    main(args.base_url)
